### 스크럼 
- 학습 목표 1: 린캔버스, 페르소나, 워크플로우 등 작성
- 학습 목표 2: 인프런 강의 1섹션 듣기
- 학습 목표 3: 커밋 메세지로 TIL 작성 어떻게 할건지 알아보기

#### MVP 단계에서는 GPT 같은 기성 LLM을 사용 -> 디벨롭 단계에서는 자체 모델 생성
## "사전학습된 공개 LLM을 파인튜닝해서, API 호출 없이 로컬이나 자체 서버에서 돌아가는 모델을 만든다"
- 사전학습 모델을 파인튜닝(fine-tuning)하는 방식 </br>
  오픈된 LLM (예: Mistral, LLaMA, Phi 등) 중 하나를 기반으로, </br>
  커밋 메시지 → TIL로 매핑된 데이터셋을 수집/정제한 다음 그걸로 특화 파인튜닝을 진행. </br>
  이렇게 되면, 오픈모델이 커밋 메시지 기반 TIL 작성에 더 잘 맞는 뇌를 갖게 됌. </br>
  
  📌 이 방식은 GPT 수준의 퍼포먼스는 아니지만 경량화, 로컬 배포 가능, 도메인 특화 성능 향상 같은 장점 가짐.

### 지금부터 공부하면 좋은 것
- 사전학습 언어모델 파인튜닝 & 서빙 </br>
  <img width="758" alt="image" src="https://github.com/user-attachments/assets/f0ea2aee-c83d-41a5-a2a0-dfd2b881a304" /> </br>

- 커밋 메시지 → TIL task 이해 & 데이터 구축 </br>
  <img width="758" alt="image" src="https://github.com/user-attachments/assets/84c03dcd-f819-48f2-bb42-4634d79a90ae" /> </br>

### 오늘의 도전 과제와 해결 방법
- mvp 이후에 오픈된 LLM (Mistral, LLaMA, Phi 등) 중 하나를 기반으로 파인튜닝해서 자체 모델을 사용한다면 모델 학습용 데이터셋 구축은 어떻게 할 것인가? </br>
  -> GitHub에서 커밋 메시지나 코드 diff를 수집해서 가상의 TIL로 생성하거나, 실제 유저들의 TIL 레포를 크롤링해서 병렬 데이터로 만들 수 있음.
- Transformer 모델의 작동 방식 </br>
  pipeline()텍스트 생성 및 분류와 같은 NLP 작업을 해결하기 위해 함수를 사용하는 방법 </br>
  Transformer 아키텍처에 대하여 </br>
  인코더, 디코더, 인코더-디코더 아키텍처와 사용 사례를 구별하는 방법 </br>
- Hugging Face Hub 의 모델을 사용하는 방법
- 데이터 세트에서 미세 조정하는 방법
- Hub에서 결과를 공유하는 방법

### 오늘의 회고
- 할게 많은데, 할게 없다는 생각이 들었다. To-do list를 글로 적고 하나씩 수행하는 방식을 사용해야겠다.
- 돈을 아끼자 !

### 참고 자료 및 링크
- [허깅페이스 트랜스포머](https://huggingface.co/docs/transformers/index)
- [허깅페이스 트랜스포머 튜토리얼](https://huggingface.co/learn/nlp-course/chapter1/1)
- [허깅페이스 깃허브](https://github.com/huggingface/transformers/tree/main/examples)
