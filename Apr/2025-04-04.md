### 스크럼 
- 학습 목표 1: hugging face transformer 튜토리얼
- 학습 목표 2: 페르소나, IA 작성 
- 학습 목표 3: 코딩 테스트 2문제 풀기
- 학습 목표 4: tensorflow&keras -> Pytorch 변환 이해 (시간이 된다면)

#### 오픈소스 LLM 모델 (파인튜닝/사용하기 좋은 모델)
1. Google Gemma (2B, 7B)
  경량 모델이라 파인튜닝과 배포가 쉬움 </br>
  Hugging Face에 공식 공개됨 </br>
  정리/요약 태스크에 적합 </br>
  google/gemma-2b 또는 gemma-7b 

2. Mistral 7B / Mixtral
  성능 좋고 빠름 (Transformer 구조 최적화) </br>
  Mixtral은 MoE(Mixture of Experts)로 메모리 절약 가능 </br>
  커뮤니티에서도 파인튜닝 예제 많음 

3. Meta LLaMA 2 / LLaMA 3 
  LLaMA 2 7B는 튜토리얼과 생태계가 풍부 </br>
  충분한 자원이 있다면 LLaMA 2 13B도 고려 가능 </br>
  최근 LLaMA 3 베타도 일부 릴리스됨

4. DeepSeek-V2
  최근 평가에서 GPT-3.5급 성능으로 주목받음 </br>
  한글 지원도 준수한 편 </br>
  텍스트 생성 성능 좋음

5. Phi-2 (MS)
  파라미터 수는 적지만 성능은 놀라움 (2.7B) </br>
  학습량 적고 자원이 적은 경우 유리 </br>
  작은 프로젝트나 교육용에 적합

### 오늘의 회고 
- 팀 프로젝트 기획서 작성중 부여된 업무는 끝냈다. 강의를 들으면서 더블체크하는 것도 좋을 것 같다.

### 참고 자료 및 링크
- [commitmate github](https://github.com/moolmin/commit-mate)
- [commitmate](https://disquiet.io/product/commit-mate)
