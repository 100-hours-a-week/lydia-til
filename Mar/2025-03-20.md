### 스크럼 
- 학습 목표 1 : 자소서 작성하기
- 학습 목표 2 : 개인 프로젝트 보고서 작성 시작
- 학습 목표 3 : CNN 모델 경량화 연구 

### 새로 배운 내용
#### 주제 1: Vegetable Image Dataset CNN research
- 상세 내용 1 : ResNet50V2는 비교적 무거운 모델이므로, 이를 작은 모델(예: MobileNetV2)에 지식을 전달하는 방식으로 Knowledge Distillation 시도 </br>
  💡 방법: 기존 모델(Teacher)로 예측한 소프트 타겟을 작은 모델(Student)에게 학습시키기

  ## "Knowledge Distillation(지식 증류)" vs "Model Pruning & Quantization(모델 경량화)" 차이
  Knowledge Distillation: 큰 모델(Teacher)의 지식을 작은 모델(Student)에게 전달하는 방식 → 아예 새로운 작은 모델을 학습하는 방법 </br>
  Model Pruning & Quantization: 기존의 큰 모델을 그대로 유지하면서 불필요한 가중치를 제거하거나 데이터 타입을 압축하여 크기를 줄이는 방법 </br>
  📍 Knowledge Distillation은 새로운 모델을 학습하는 방식이고, Model Pruning & Quantization은 기존 모델을 최적화하는 방식
  
- 상세 내용 2 : Flask를 사용해 REST API 서버를 만들어 클라우드 기반 IDE에서 실행

### 오늘의 도전 과제와 해결 방법
- 도전 과제 1: 경량화 실행 중 에포크마다 커스텀 학습 루프로 모든 배치를 매 배치마다 Teacher(ResNet50V2)도 추론 -> 학습 시간이 많이 걸림 </br>
  Teacher 예측을 매번 하지 않고 미리 추론(offline) 하는 방식 사용 </br>
  데이터셋 전체에 대해 Teacher 예측을 한 번만 미리 수행한 뒤, 결과(소프트 라벨)를 저장해두고 Student가 그 라벨을 학습 </br>
  
- 도전 과제 2: 무거운 모델인 ResNet을 지식전달하여 가벼운 모델인 MobileNet으로 경량화하고 실행했는데 학습 시간이 너무 오래걸림.
  지식 증류의 목적:
  Teacher의 성능(지식)을 Student가 최대한 이어받아 최종 모델을 작고 빠르게 만드는 것.
  => 학습 단계를 빠르게 해주는 것은 아님.
  => 최종 추론(Inference) 시간이 빨라진다.
  
  온라인 Distillation:
  Teacher와 Student 동시에 배치 처리
  학습 시간이 길어질 수 있음 (특히 Teacher가 큰 모델이면 더욱).
  
  오프라인 Distillation:
  Teacher 추론을 미리 해두고, 저장된 logits으로 학습
  학습 시 Teacher를 호출하지 않아 학습 시간을 단축할 수 있음.

### 참고 자료 및 링크
- [링크 제목](URL)
- [링크 제목](URL)
