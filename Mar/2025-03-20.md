### 스크럼 
- 학습 목표 1 : 자소서 작성하기
- 학습 목표 2 : 개인 프로젝트 보고서 작성 시작
- 학습 목표 3 : CNN 모델 경량화 연구 

### 새로 배운 내용
#### 주제 1: Vegetable Image Dataset CNN research
- 상세 내용 1 : ResNet50V2는 비교적 무거운 모델이므로, 이를 작은 모델(예: MobileNetV2)에 지식을 전달하는 방식으로 Knowledge Distillation 시도 </br>
  💡 방법: 기존 모델(Teacher)로 예측한 소프트 타겟을 작은 모델(Student)에게 학습시키기

  ## "Knowledge Distillation(지식 증류)" vs "Model Pruning & Quantization(모델 경량화)" 차이
  Knowledge Distillation: 큰 모델(Teacher)의 지식을 작은 모델(Student)에게 전달하는 방식 → 아예 새로운 작은 모델을 학습하는 방법 </br>
  Model Pruning & Quantization: 기존의 큰 모델을 그대로 유지하면서 불필요한 가중치를 제거하거나 데이터 타입을 압축하여 크기를 줄이는 방법 </br>
  📍 Knowledge Distillation은 새로운 모델을 학습하는 방식이고, Model Pruning & Quantization은 기존 모델을 최적화하는 방식
  
- 상세 내용 2 : Flask를 사용해 REST API 서버를 만들어 클라우드 기반 IDE에서 실행

### 오늘의 도전 과제와 해결 방법
- 도전 과제 1: 도전 과제에 대한 설명 및 해결 방법
- 도전 과제 2: 도전 과제에 대한 설명 및 해결 방법

### 오늘의 회고
- 오늘의 학습 경험에 대한 자유로운 생각이나 느낀 점을 기록합니다.
- 성공적인 점, 개선해야 할 점, 새롭게 시도하고 싶은 방법 등을 포함할 수 있습니다.

### 참고 자료 및 링크
- [링크 제목](URL)
- [링크 제목](URL)
