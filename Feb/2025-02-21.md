### 스크럼
- 학습 목표 1 : 과제 3번  
- 학습 목표 2 : 미니퀘스트 데이터 전처리~머신러닝
- 학습 목표 3 : 딥다이브 정리
---
### 새로 배운 내용
---
#### 주제 1: 퍼셉트론의 작동 원리와 이진 분류 문제 해결 과정

퍼셉트론:  인공 신경망의 가장 기본적인 구성 요소로, 주로 이진 분류 문제를 해결하는 데 사용

### 퍼셉트론의 기본 구조 및 작동 원리

$$ y=f(W⋅X+b)$$

- **입력 값 (Inputs):** 입력 벡터
    
    x1,x2, ... ,xn
    
- **가중치 (Weights):** 각 입력에 곱해지는 값, 해당 입력의 중요도를 나타낸다.
    
    w1,w2, ... ,wn
    
- **편향 (Bias):** 
추가적인 상수항으로, 계산된 값이 임계값을 넘기 쉽게 도와준다.

    b

- **활성화 함수 (Activation Function):**
    
    주로 계단 함수(또는 임계값 함수)를 사용하여, 계산된 결과를 이진 출력(0 또는 1)으로 변환한다.

### 작동 원리

1. **선형 결합 계산**
    
    각 입력과 가중치를 곱한 후 모두 더하고, 편향을 더한다.
    
    합계=w1x1+w2x2+⋯+wnxn+b
    
2. **활성화 함수 적용**
    
    계산된 합계를 계단 함수에 통과시킨다.
    
    - if 합계≥0 → 출력은 1, else → 출력은 0
    - 입력 데이터가 어느 클래스에 속하는지 결정하게 된다.

### 이진 분류 문제 해결 과정

1. **데이터 준비**
    - 학습 데이터셋은 두 개의 클래스(0과 1)로 구성.
    - 각 데이터 포인트는 특징(feature) 벡터와 정답 레이블로 이루어진다.
    
2. **가중치(W)와 바이어스(b) 초기화**
    - 가중치와 편향을 임의의 값으로 초기화.
    
3. **예측**
    - 각 데이터 포인트에 대해 위의 선형 결합과 활성화 함수를 적용하여 예측 값을 계산.
    
4. **오차 계산 및 가중치 업데이트**
    - 예측 값과 실제 정답을 비교해 오차(error)를 구한다.
    - 오차가 발생하면, 가중치와 편향 업데이트
        
        
5. **반복 학습:**
    - 전체 데이터셋에 대해 위 과정을 반복하며, 모든 데이터가 올바르게 분류되거나 정해진 반복 횟수만큼 학습
6. **분류 결정:**
    - 학습이 완료되면, 퍼셉트론은 학습 데이터의 패턴에 맞춰 최적의 가중치와 편향을 갖게 된다.
    - 새로운 입력 데이터가 들어오면, 계산된 선형 결합 값이 0 이상이면 한 클래스(1), 0 미만이면 다른 클래스(0)로 분류된다.

### 한줄정리

- **퍼셉트론**은 입력에 가중치를 부여해 선형 결합을 계산하고, 이를 계단 함수를 통해 이진 결과로 변환한다.
- **학습 과정**에서는 데이터의 오차를 기반으로 가중치와 편향을 조정하여, 점진적으로 올바른 분류 경계를 형성한다.
- **이진 분류 문제**에서는 이러한 과정을 통해 두 클래스 사이를 구분하는 결정 경계를 학습하게 된다.

---
#### 주제 2:  k-최근접이웃 알고리즘이 데이터의 패턴을 이해하는 방식에 대해 설명


### K-NN이란?

- 지도 학습(Supervised Learning) 방식의 대표적인 거리 기반 알고리즘으로
**“유사한 데이터는 가까운 위치에 모인다”** 라는 가정에 기반한 방식
- 분류(Classification)나 회귀(Regression) 문제에서 널리 사용
- **비모수적(Non-parametric) 접근:** 미리 수식(모델)을 만들지 않고, 가지고 있는 데이터를 그대로 두고 필요한 순간에 거리로 판단

> 즉, 새로운 데이터 포인트가 주어졌을 때, 가장 가까운 K개의 데이터를 참고하여 해당 데이터의 속성을 결정한다.
> 

### 동작 원리

1. **데이터 저장:** 학습단계에서 복잡한 모델을 만들지 않고, 그저 주어진(학습) 데이터를 그대로 저장
2. **새 데이터 등장:** 예측하고 싶은 데이터 포인트(샘플)가 새로 들어옴
3. **거리 계산:** 새로운 데이터와 기존 데이터 사이의 거리를 계산
4. **K개의 이웃 선택:** 가장 가까운 K개의 데이터를 찾음
5. **예측(분류/회귀):**
    - 분류 → K개 중 가장 많이 등장하는 클래스로 결정 (다수결)
    - 회귀 → K개의 값의 평균 또는 가중 평균으로 결정
6. **결과 출력:** 최종 예측 값(또는 클래스)을 반환

### 예제

### **💡 분류 문제 예제**

붓꽃(Iris) 데이터셋에서 꽃잎과 꽃받침의 길이를 이용하여 붓꽃의 품종을 예측하는 경우:

- 새로운 샘플의 꽃잎과 꽃받침 길이를 기존 데이터와 비교.
- K개의 최근접 이웃을 찾고, 가장 많은 품종으로 예측.
- 예를 들어, 3-NN을 사용하면 가장 가까운 3개의 데이터 중 다수결을 통해 품종을 결정.

### **💡 회귀 문제 예제**

주택 가격 예측 문제에서 특정 지역의 새로운 집 가격을 예측하는 경우:

- 새로운 집과 기존 주택 데이터 간의 거리 측정.
- K개의 가까운 이웃 집을 선택한 후, 이들의 평균 가격을 예측값으로 반환.

### 거리를 사용하는 이유

- “가까운 데이터일수록 서로 닮았다”고 가정
- 일반적으로 **유클리드 거리**(직선 거리)를 많이 쓰지만, **맨해튼 거리**, **코사인 유사도** 등 상황에 따라 다른 방법도 사용 가능

K값은 어떻게 정하나요?

- **K가 작으면(예: 1)**: 노이즈(이상치)에 민감 → 과적합(overfitting) 위험
- **K가 크면(예: 100)**: 데이터의 세부 패턴을 놓칠 위험 → 과소적합(underfitting)
- 적절한 K를 찾기 위해 교차 검증(cross-validation) 등을 통해 성능이 가장 좋은 값을 선택
- 분류 문제에서는 보통 홀수를 주로 사용(다수결에서 동점 방지 목적)

### 장점과 한계

### ⭕️ 장점

1. 구현이 간단하고 직관적임.
2. 선형적 관계가 아닌 복잡한 패턴도 학습 가능.
3. 새로운 데이터에 적응하는 유연한 방식.
    - 미리 모델을 고정하지 않으므로 데이터가 새로 들어와도 학습 절차가 간단
    

### ❌ 한계

1. 계산량↑: 데이터가 많아질수록 거리 계산 비용이 커진다.
2. 고차원 문제에서 성능 저하: 차원의 저주(Curse of Dimensionality) 문제 발생 위험
3. 최적의 K 값 선택 필요: 너무 작으면 과적합, 너무 크면 일반화가 어려움

### 한줄정리

"K-NN은 데이터가 ‘가까우면 닮았다’는 간단한 원리를 활용해, 저장된 데이터와의 거리를 기준으로 분류나 회귀를 수행하는 알고리즘"

---
### 오늘의 회고
- 딥다이브 주제 2개는 조금 힘들었다. 딥하지 않게 완성하는 느낌이랄까
- 미니퀘스트, 과제를 빨리 해서 다른 공부를 할 수 있는 시간을 늘리면 좋을 것 같다.
---
### 참고 자료 및 링크
- [X](URL)
